# atlasmtl Current Project Summary (2026-02-28)

## Current positioning

From a paper perspective, `atlasmtl` now has a clear and defensible method
position:

- the main task is `sc -> sc` reference mapping
- the main output is multi-level label transfer
- the main claim is not “best integrated embedding”
- the main claim is:
  - labels are accurate
  - uncertainty is explicit
  - rejection is possible
  - difficult cells can be rescued with KNN support
  - hierarchy consistency can be enforced
  - the full workflow is reproducible and traceable

This positioning is now consistent across the method design, benchmark
protocol, documentation, and comparator strategy.

## Method design in the current paper framing

atlasmtl is best described as a framework with four layers:

### 1. Core model

- shared encoder
- one label head per annotation level
- optional coordinate heads

This is the main multi-task learning component and remains the backbone of the
method.

### 2. Reliability layer

- confidence
- margin
- calibration
- open-set / Unknown

This layer supports the paper’s reliability claim and distinguishes atlasmtl
from plain forced-label transfer systems.

### 3. Structural enhancement layer

- KNN rescue
- hierarchy enforcement
- coordinate-aware support

This layer explains how atlasmtl handles difficult cells, label path
consistency, and atlas-aligned geometric context.

### 4. Engineering and traceability layer

- manifest-based artifact loading
- checksums
- train/predict/benchmark run manifests
- benchmark protocol and report generation

This layer is important for reproducibility and practical deployment.

## What is already completed

The following parts are already in place and are strong enough to support a
paper draft.

### Method and engineering

- main training / prediction pipeline
- public API and artifact contract
- calibration and open-set support
- KNN correction variants
- hierarchy enforcement
- optional domain and topology extensions
- manifest/checksum/run-manifest traceability

### Benchmark infrastructure

- benchmark runner
- structured output contract
- markdown report generation
- paper-table export utilities
- comparator protocol template

### Comparator set

Current runnable benchmark methods:

- `atlasmtl`
- `reference_knn`
- `celltypist`
- `scanvi`
- `singler`
- `symphony`
- `azimuth`

### Documentation and positioning

- research positioning is written down
- architecture and overall summary are documented
- protocol is documented
- comparator fairness boundary is documented

## What is done but still not paper-final

Several parts are implemented, but still need to be tightened before they can
support a final result section.

### 1. Formal benchmark execution

The benchmark system can run, but formal result generation still requires:

- fixed datasets
- fixed reference/query split rules
- fixed target label levels
- fixed comparator hyperparameter policy

### 2. Native Azimuth in formal runs

`azimuth` now supports a native path, but tiny toy tests may fall back to a
Seurat anchor-transfer backend. This is acceptable for engineering smoke
testing, but formal main tables should only use native `Azimuth` results.

### 3. atlasmtl ablation

The framework now contains enough components that a formal paper needs explicit
ablation for:

- base MTL
- + calibration
- + open-set / Unknown
- + KNN rescue
- + hierarchy

This is not yet the main reported result set.

### 4. Domain robustness protocol

Grouped reporting exists, but the protocol for in-domain / cross-domain /
held-out settings is not yet fully standardized.

## What still needs optimization

The main optimization target is no longer basic implementation. It is
experimental discipline.

The highest-priority improvements are:

- freeze the formal benchmark datasets and splits
- freeze the shared target label level for comparator fairness
- freeze the hyperparameter policy for each comparator
- produce atlasmtl ablation tables
- produce the first formal benchmark result bundle

## What is still not started or not part of the current paper

The following items should not define the current paper scope:

- query-time adaptation
- integration-first scIB benchmark framing
- stronger domain adaptation beyond the current lightweight penalty
- full comparator matrix orchestration across many settings
- deconvolution / localization / gene-imputation task expansion

These can remain future directions.

## Recommended next step

The next step should be experimental convergence, not further feature sprawl.

Recommended order:

1. finalize official benchmark datasets and splits
2. finalize comparator hyperparameter policy
3. run formal comparator benchmarks
4. run atlasmtl ablations
5. assemble paper tables and narrative around the fixed benchmark

## Macro stage assessment

The project is currently at this stage:

- method skeleton: completed
- engineering and protocol baseline: completed
- comparator infrastructure: completed
- formal benchmark production: next critical stage
- future advanced extensions: intentionally deferred

So the project should now shift from “keep adding features” to “lock the
experiment design and generate paper-grade results”.
